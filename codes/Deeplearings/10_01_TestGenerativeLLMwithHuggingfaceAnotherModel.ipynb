{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.40.1 bitsandbytes==0.43.1 accelerate==0.29.3 datasets==2.19.0 tiktoken==0.6.0 -qqq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:42:06.818545Z","iopub.execute_input":"2025-02-10T01:42:06.818884Z","iopub.status.idle":"2025-02-10T01:42:26.764517Z","shell.execute_reply.started":"2025-02-10T01:42:06.818857Z","shell.execute_reply":"2025-02-10T01:42:26.763756Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.3.1 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install --upgrade huggingface-hub -qqq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:42:26.765648Z","iopub.execute_input":"2025-02-10T01:42:26.765908Z","iopub.status.idle":"2025-02-10T01:42:26.769261Z","shell.execute_reply.started":"2025-02-10T01:42:26.765874Z","shell.execute_reply":"2025-02-10T01:42:26.768617Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:42:26.770569Z","iopub.execute_input":"2025-02-10T01:42:26.770842Z","iopub.status.idle":"2025-02-10T01:42:26.786982Z","shell.execute_reply.started":"2025-02-10T01:42:26.770817Z","shell.execute_reply":"2025-02-10T01:42:26.786190Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 기존 모델 활용","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM # LLM 모델 loading","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:42:26.788141Z","iopub.execute_input":"2025-02-10T01:42:26.788377Z","iopub.status.idle":"2025-02-10T01:42:31.375060Z","shell.execute_reply.started":"2025-02-10T01:42:26.788352Z","shell.execute_reply":"2025-02-10T01:42:31.374436Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"beomi/Yi-Ko-6B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"beomi/Yi-Ko-6B\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:42:31.375787Z","iopub.execute_input":"2025-02-10T01:42:31.376125Z","iopub.status.idle":"2025-02-10T01:48:15.717775Z","shell.execute_reply.started":"2025-02-10T01:42:31.376103Z","shell.execute_reply":"2025-02-10T01:48:15.713532Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66c565567bf4eb098bb3be55c4a8122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.28M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89b7fac842f4e44b24c5748f34a7d70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/573 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cebb214584e4d90bae6aa0341a92198"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74d6f92b47894c178ab024f248744ba3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0189c3f8c61c4d4389008387ea85a796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab00740c084845468d99311b1ce82294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"228aa185a42c47bdb0fd2e098b2f8f06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/2.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1773fc6d72e84c2c991414c0fbbd861f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4e10064b0ef48f5a3a8a68955626891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/2.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce7d218e53a04e11840837d611fe5e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/643M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af2ef11d0fa460996f348a197d8f487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"385f1abe521147b7b083d2f1cd7d2036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931cf97033e44ae19ded66ded5d0e299"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"DDL = 'CREATE TABLE players ( player_id INT PRIMARY KEY AUTO_INCREMENT, username VARCHAR(255) UNIQUE NOT NULL, email VARCHAR(255) UNIQUE NOT NULL, password_hash VARCHAR(255) NOT NULL, date_joined DATETIME NOT NULL, last_login DATETIME );'\nQuestion = '모든 플레이어 정보를 조회해 줘'\nSQL = ''\nexample = f'''\n    당신은 sql 생성하는 전문가야.\n    DDL 활용해 Question 해결하는 SQL query 생성\n    추측 과정은 빼고 SQL만 출력 \n    {DDL}\n    {Question}\n    {SQL}\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:48:15.723240Z","iopub.execute_input":"2025-02-10T01:48:15.723568Z","iopub.status.idle":"2025-02-10T01:48:15.731676Z","shell.execute_reply.started":"2025-02-10T01:48:15.723547Z","shell.execute_reply":"2025-02-10T01:48:15.731091Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenizer.eos_token_id, tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:48:15.732299Z","iopub.execute_input":"2025-02-10T01:48:15.732509Z","iopub.status.idle":"2025-02-10T01:48:15.777494Z","shell.execute_reply.started":"2025-02-10T01:48:15.732490Z","shell.execute_reply":"2025-02-10T01:48:15.776883Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(2, 0)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"## 토크나이징\ninputs = tokenizer.encode(example, return_tensors='pt')\nlen(inputs[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:48:15.779531Z","iopub.execute_input":"2025-02-10T01:48:15.779726Z","iopub.status.idle":"2025-02-10T01:48:15.878297Z","shell.execute_reply.started":"2025-02-10T01:48:15.779707Z","shell.execute_reply":"2025-02-10T01:48:15.877664Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"132"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 모델 예측 수행(생성 글 작성)\noutputs = model.generate(inputs\n              , max_new_tokens=100\n              , pad_token_id = tokenizer.eos_token_id\n              , temperature=0.5)\noutputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:48:15.879124Z","iopub.execute_input":"2025-02-10T01:48:15.879370Z","iopub.status.idle":"2025-02-10T01:50:05.803823Z","shell.execute_reply.started":"2025-02-10T01:48:15.879336Z","shell.execute_reply":"2025-02-10T01:50:05.802933Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[    1, 59568,   144,   139, 76851, 71613, 33162, 64069, 76858, 64018,\n         65475, 76848,    98,   144,   139, 14499, 59620, 69033, 24983, 65192,\n         64018, 16292,  8975, 64069, 76858,   144,   139, 76992, 77248, 64230,\n         65028, 67533, 16292, 76822, 64179, 76963, 59568,   144,   139, 57739,\n         30589,  3411,   662,  3875, 59593,   619, 11710,  6123,  3811, 15914,\n         34341, 58552, 59593,  1625, 26406, 12766,    97, 20576,  1008, 23778,\n          1955, 59605,    79,    82,    82, 59604,  8568, 59597, 24849,  5928,\n          8987,    97,  3449,  1008, 23778,  1955, 59605,    79,    82,    82,\n         59604,  8568, 59597, 24849,  5928,  8987,    97,  8355, 59593, 14529,\n          1008, 23778,  1955, 59605,    79,    82,    82, 59604,  5928,  8987,\n            97,  3817, 59593,  9522,  1559,   723,  1765,  3013, 13862,  5928,\n          8987,    97,  1426, 59593, 17817,   723,  1765,  3013, 13862, 58946,\n           144,   139, 71386, 67763, 76816, 68724, 72178, 63934, 69305,   144,\n           139,   144,   139, 27683,  1307, 17509,  3411, 59631,   144,   139,\n         71386, 67763, 76816, 68724, 72178, 63934, 69305,   144,   139,   144,\n           139, 27683,  1307, 17509,  3411, 31686, 20576,   762,  1111, 59617,\n          1577, 32958,   144,   139, 59610, 59617,  1577, 59610, 64291, 76607,\n         68724, 72178, 63934, 69305,   144,   139,   144,   139, 27683,  1307,\n         17509,  3411, 31686, 20576,   762,  1111, 59617,  1577, 59610,  6101,\n          8355, 59593, 14529,   762,  1111,    78,    79,    80,    81,    82,\n            83,    84,    85, 32958,   144,   139, 59610, 59617,  1577, 59610,\n         64291, 76607, 68724, 72178, 63934, 69305,  6101,  1111,    78,    79,\n            80,    81,    82,    83,    84,    85, 59610, 64628, 73357, 77098,\n         66287, 66040]])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# 생성된 텍스트 출력\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nresult","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:50:05.804776Z","iopub.execute_input":"2025-02-10T01:50:05.805113Z","iopub.status.idle":"2025-02-10T01:50:05.811876Z","shell.execute_reply.started":"2025-02-10T01:50:05.805080Z","shell.execute_reply":"2025-02-10T01:50:05.811016Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\" \\n    당신은 sql 생성하는 전문가야.\\n    DDL 활용해 Question 해결하는 SQL query 생성\\n    추측 과정은 빼고 SQL만 출력 \\n    CREATE TABLE players ( player_id INT PRIMARY KEY AUTO_INCREMENT, username VARCHAR(255) UNIQUE NOT NULL, email VARCHAR(255) UNIQUE NOT NULL, password_hash VARCHAR(255) NOT NULL, date_joined DATETIME NOT NULL, last_login DATETIME );\\n    모든 플레이어 정보를 조회해 줘\\n    \\n    SELECT * FROM players;\\n    모든 플레이어 정보를 조회해 줘\\n    \\n    SELECT * FROM players WHERE username = 'james';\\n    'james'라는 사용자 정보를 조회해 줘\\n    \\n    SELECT * FROM players WHERE username = 'james' AND password_hash = '12345678';\\n    'james'라는 사용자 정보를 조회해 줘 AND '12345678'이라는 패스워드를 가진\""},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### 모델은 한번에 동작하게 작성","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig, pipeline\n\ndef make_inference_pipeline(model_id):\n    # 토크나이징\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    \n    # 모델\n    model = AutoModelForCausalLM.from_pretrained(model_id\n                                                , load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16  # 양자화 정의\n                                                , device_map=\"auto\")\n    # pipeline : 예측 초기화 설정\n    pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n    return pipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:50:05.812737Z","iopub.execute_input":"2025-02-10T01:50:05.812947Z","iopub.status.idle":"2025-02-10T01:50:21.289801Z","shell.execute_reply.started":"2025-02-10T01:50:05.812928Z","shell.execute_reply":"2025-02-10T01:50:21.289162Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model_id = \"beomi/Yi-Ko-6B\"\nhf_pipe = make_inference_pipeline(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T01:50:21.290524Z","iopub.execute_input":"2025-02-10T01:50:21.291076Z","iopub.status.idle":"2025-02-10T01:51:19.203972Z","shell.execute_reply.started":"2025-02-10T01:50:21.291022Z","shell.execute_reply":"2025-02-10T01:51:19.203231Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f15dde5350844ef2a22f04e3ca8b01bb"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"results = hf_pipe(example, do_sample=False,\n    return_full_text=False, max_length=512, truncation=True)\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T02:12:21.280872Z","iopub.execute_input":"2025-02-10T02:12:21.281226Z","iopub.status.idle":"2025-02-10T02:12:46.303928Z","shell.execute_reply.started":"2025-02-10T02:12:21.281197Z","shell.execute_reply":"2025-02-10T02:12:46.303243Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': \"    SELECT * FROM players;\\n    \\n    SELECT * FROM players WHERE username = 'jonghoon';\\n    \\n    SELECT * FROM players WHERE email = 'jonghoon@gmail.com';\\n    \\n    SELECT * FROM players WHERE password_hash = '12345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\"}]"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"type(hf_pipe), type(pipeline)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T02:05:40.356947Z","iopub.execute_input":"2025-02-10T02:05:40.357287Z","iopub.status.idle":"2025-02-10T02:05:40.362164Z","shell.execute_reply.started":"2025-02-10T02:05:40.357258Z","shell.execute_reply":"2025-02-10T02:05:40.361271Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(transformers.pipelines.text_generation.TextGenerationPipeline, function)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"type(results), type(results[0]), results[0].keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T02:14:16.096671Z","iopub.execute_input":"2025-02-10T02:14:16.096954Z","iopub.status.idle":"2025-02-10T02:14:16.102074Z","shell.execute_reply.started":"2025-02-10T02:14:16.096929Z","shell.execute_reply":"2025-02-10T02:14:16.101308Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(list, dict, dict_keys(['generated_text']))"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"results[0]['generated_text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T02:15:42.846528Z","iopub.execute_input":"2025-02-10T02:15:42.846846Z","iopub.status.idle":"2025-02-10T02:15:42.852022Z","shell.execute_reply.started":"2025-02-10T02:15:42.846820Z","shell.execute_reply":"2025-02-10T02:15:42.851130Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"\"    SELECT * FROM players;\\n    \\n    SELECT * FROM players WHERE username = 'jonghoon';\\n    \\n    SELECT * FROM players WHERE email = 'jonghoon@gmail.com';\\n    \\n    SELECT * FROM players WHERE password_hash = '12345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\""},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}